import os
import random
import numpy as np
import torch
import pandas as pd
from torch import optim, nn
from sklearn.model_selection import train_test_split

from dataset import create_data_loader, train_preprocess, val_preprocess
from model import load_model
from utils import add_absolute_paths, add_label_index, freeze_parameters
from train import train_model
from parser import parse_args

# Parse arguments
args = parse_args()

# Function to set random seed for reproducibility
def set_seed(seed=args.random_seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


# Set the seed for reproducibility
set_seed(args.random_seed)

device = args.device if torch.cuda.is_available() else 'cpu'

# Load and preprocess data
data = pd.read_csv(args.data_path)
data = add_absolute_paths(data, args.data_root)
data = add_label_index(data)

x_train, x_test, y_train, y_test = train_test_split(
    data, data['label_index'], test_size=args.test_size, random_state=args.random_state, stratify=data['label_index']
)

train_dataloader = create_data_loader(x_train, train_preprocess, batch_size=args.batch_size)
val_dataloader = create_data_loader(x_test, val_preprocess, batch_size=args.batch_size)

# Load model
model, preprocess = load_model(device, args)
# 동결하지 않을 파라미터 이름들
frozen_params = ['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias','model.visual.transformer.resblocks.12.attn.in_proj_weight', 'model.visual.transformer.resblocks.12.attn.in_proj_bias', 'model.visual.transformer.resblocks.12.attn.out_proj.weight', 'model.visual.transformer.resblocks.12.attn.out_proj.bias', 'model.visual.transformer.resblocks.12.ln_1.weight', 'model.visual.transformer.resblocks.12.ln_1.bias', 'model.visual.transformer.resblocks.12.mlp.c_fc.weight', 'model.visual.transformer.resblocks.12.mlp.c_fc.bias', 'model.visual.transformer.resblocks.12.mlp.c_proj.weight', 'model.visual.transformer.resblocks.12.mlp.c_proj.bias', 'model.visual.transformer.resblocks.12.ln_2.weight', 'model.visual.transformer.resblocks.12.ln_2.bias', 'model.visual.transformer.resblocks.13.attn.in_proj_weight', 'model.visual.transformer.resblocks.13.attn.in_proj_bias', 'model.visual.transformer.resblocks.13.attn.out_proj.weight', 'model.visual.transformer.resblocks.13.attn.out_proj.bias', 'model.visual.transformer.resblocks.13.ln_1.weight', 'model.visual.transformer.resblocks.13.ln_1.bias', 'model.visual.transformer.resblocks.13.mlp.c_fc.weight', 'model.visual.transformer.resblocks.13.mlp.c_fc.bias', 'model.visual.transformer.resblocks.13.mlp.c_proj.weight', 'model.visual.transformer.resblocks.13.mlp.c_proj.bias', 'model.visual.transformer.resblocks.13.ln_2.weight', 'model.visual.transformer.resblocks.13.ln_2.bias', 'model.visual.transformer.resblocks.14.attn.in_proj_weight', 'model.visual.transformer.resblocks.14.attn.in_proj_bias', 'model.visual.transformer.resblocks.14.attn.out_proj.weight', 'model.visual.transformer.resblocks.14.attn.out_proj.bias', 'model.visual.transformer.resblocks.14.ln_1.weight', 'model.visual.transformer.resblocks.14.ln_1.bias', 'model.visual.transformer.resblocks.14.mlp.c_fc.weight', 'model.visual.transformer.resblocks.14.mlp.c_fc.bias', 'model.visual.transformer.resblocks.14.mlp.c_proj.weight', 'model.visual.transformer.resblocks.14.mlp.c_proj.bias', 'model.visual.transformer.resblocks.14.ln_2.weight', 'model.visual.transformer.resblocks.14.ln_2.bias', 'model.visual.transformer.resblocks.15.attn.in_proj_weight', 'model.visual.transformer.resblocks.15.attn.in_proj_bias', 'model.visual.transformer.resblocks.15.attn.out_proj.weight', 'model.visual.transformer.resblocks.15.attn.out_proj.bias', 'model.visual.transformer.resblocks.15.ln_1.weight', 'model.visual.transformer.resblocks.15.ln_1.bias', 'model.visual.transformer.resblocks.15.mlp.c_fc.weight', 'model.visual.transformer.resblocks.15.mlp.c_fc.bias', 'model.visual.transformer.resblocks.15.mlp.c_proj.weight', 'model.visual.transformer.resblocks.15.mlp.c_proj.bias', 'model.visual.transformer.resblocks.15.ln_2.weight', 'model.visual.transformer.resblocks.15.ln_2.bias', 'model.visual.transformer.resblocks.16.attn.in_proj_weight', 'model.visual.transformer.resblocks.16.attn.in_proj_bias', 'model.visual.transformer.resblocks.16.attn.out_proj.weight', 'model.visual.transformer.resblocks.16.attn.out_proj.bias', 'model.visual.transformer.resblocks.16.ln_1.weight', 'model.visual.transformer.resblocks.16.ln_1.bias', 'model.visual.transformer.resblocks.16.mlp.c_fc.weight', 'model.visual.transformer.resblocks.16.mlp.c_fc.bias', 'model.visual.transformer.resblocks.16.mlp.c_proj.weight', 'model.visual.transformer.resblocks.16.mlp.c_proj.bias', 'model.visual.transformer.resblocks.16.ln_2.weight', 'model.visual.transformer.resblocks.16.ln_2.bias', 'model.visual.transformer.resblocks.17.attn.in_proj_weight', 'model.visual.transformer.resblocks.17.attn.in_proj_bias', 'model.visual.transformer.resblocks.17.attn.out_proj.weight', 'model.visual.transformer.resblocks.17.attn.out_proj.bias', 'model.visual.transformer.resblocks.17.ln_1.weight', 'model.visual.transformer.resblocks.17.ln_1.bias', 'model.visual.transformer.resblocks.17.mlp.c_fc.weight', 'model.visual.transformer.resblocks.17.mlp.c_fc.bias', 'model.visual.transformer.resblocks.17.mlp.c_proj.weight', 'model.visual.transformer.resblocks.17.mlp.c_proj.bias', 'model.visual.transformer.resblocks.17.ln_2.weight', 'model.visual.transformer.resblocks.17.ln_2.bias', 'model.visual.transformer.resblocks.18.attn.in_proj_weight', 'model.visual.transformer.resblocks.18.attn.in_proj_bias', 'model.visual.transformer.resblocks.18.attn.out_proj.weight', 'model.visual.transformer.resblocks.18.attn.out_proj.bias', 'model.visual.transformer.resblocks.18.ln_1.weight', 'model.visual.transformer.resblocks.18.ln_1.bias', 'model.visual.transformer.resblocks.18.mlp.c_fc.weight', 'model.visual.transformer.resblocks.18.mlp.c_fc.bias', 'model.visual.transformer.resblocks.18.mlp.c_proj.weight', 'model.visual.transformer.resblocks.18.mlp.c_proj.bias', 'model.visual.transformer.resblocks.18.ln_2.weight', 'model.visual.transformer.resblocks.18.ln_2.bias', 'model.visual.transformer.resblocks.19.attn.in_proj_weight', 'model.visual.transformer.resblocks.19.attn.in_proj_bias', 'model.visual.transformer.resblocks.19.attn.out_proj.weight', 'model.visual.transformer.resblocks.19.attn.out_proj.bias', 'model.visual.transformer.resblocks.19.ln_1.weight', 'model.visual.transformer.resblocks.19.ln_1.bias', 'model.visual.transformer.resblocks.19.mlp.c_fc.weight', 'model.visual.transformer.resblocks.19.mlp.c_fc.bias', 'model.visual.transformer.resblocks.19.mlp.c_proj.weight', 'model.visual.transformer.resblocks.19.mlp.c_proj.bias', 'model.visual.transformer.resblocks.19.ln_2.weight', 'model.visual.transformer.resblocks.19.ln_2.bias', 'model.visual.transformer.resblocks.20.attn.in_proj_weight', 'model.visual.transformer.resblocks.20.attn.in_proj_bias', 'model.visual.transformer.resblocks.20.attn.out_proj.weight', 'model.visual.transformer.resblocks.20.attn.out_proj.bias', 'model.visual.transformer.resblocks.20.ln_1.weight', 'model.visual.transformer.resblocks.20.ln_1.bias', 'model.visual.transformer.resblocks.20.mlp.c_fc.weight', 'model.visual.transformer.resblocks.20.mlp.c_fc.bias', 'model.visual.transformer.resblocks.20.mlp.c_proj.weight', 'model.visual.transformer.resblocks.20.mlp.c_proj.bias', 'model.visual.transformer.resblocks.20.ln_2.weight', 'model.visual.transformer.resblocks.20.ln_2.bias', 'model.visual.transformer.resblocks.21.attn.in_proj_weight', 'model.visual.transformer.resblocks.21.attn.in_proj_bias', 'model.visual.transformer.resblocks.21.attn.out_proj.weight', 'model.visual.transformer.resblocks.21.attn.out_proj.bias', 'model.visual.transformer.resblocks.21.ln_1.weight', 'model.visual.transformer.resblocks.21.ln_1.bias', 'model.visual.transformer.resblocks.21.mlp.c_fc.weight', 'model.visual.transformer.resblocks.21.mlp.c_fc.bias', 'model.visual.transformer.resblocks.21.mlp.c_proj.weight', 'model.visual.transformer.resblocks.21.mlp.c_proj.bias', 'model.visual.transformer.resblocks.21.ln_2.weight', 'model.visual.transformer.resblocks.21.ln_2.bias', 'model.visual.transformer.resblocks.22.attn.in_proj_weight', 'model.visual.transformer.resblocks.22.attn.in_proj_bias', 'model.visual.transformer.resblocks.22.attn.out_proj.weight', 'model.visual.transformer.resblocks.22.attn.out_proj.bias', 'model.visual.transformer.resblocks.22.ln_1.weight', 'model.visual.transformer.resblocks.22.ln_1.bias', 'model.visual.transformer.resblocks.22.mlp.c_fc.weight', 'model.visual.transformer.resblocks.22.mlp.c_fc.bias', 'model.visual.transformer.resblocks.22.mlp.c_proj.weight', 'model.visual.transformer.resblocks.22.mlp.c_proj.bias', 'model.visual.transformer.resblocks.22.ln_2.weight', 'model.visual.transformer.resblocks.22.ln_2.bias', 'model.visual.transformer.resblocks.23.attn.in_proj_weight', 'model.visual.transformer.resblocks.23.attn.in_proj_bias', 'model.visual.transformer.resblocks.23.attn.out_proj.weight', 'model.visual.transformer.resblocks.23.attn.out_proj.bias', 'model.visual.transformer.resblocks.23.ln_1.weight', 'model.visual.transformer.resblocks.23.ln_1.bias', 'model.visual.transformer.resblocks.23.mlp.c_fc.weight', 'model.visual.transformer.resblocks.23.mlp.c_fc.bias', 'model.visual.transformer.resblocks.23.mlp.c_proj.weight', 'model.visual.transformer.resblocks.23.mlp.c_proj.bias', 'model.visual.transformer.resblocks.23.ln_2.weight', 'model.visual.transformer.resblocks.23.ln_2.bias', 'model.visual.ln_post.weight', 'model.visual.ln_post.bias', 'model.transformer.resblocks.6.attn.in_proj_weight', 'model.transformer.resblocks.6.attn.in_proj_bias', 'model.transformer.resblocks.6.attn.out_proj.weight', 'model.transformer.resblocks.6.attn.out_proj.bias', 'model.transformer.resblocks.6.ln_1.weight', 'model.transformer.resblocks.6.ln_1.bias', 'model.transformer.resblocks.6.mlp.c_fc.weight', 'model.transformer.resblocks.6.mlp.c_fc.bias', 'model.transformer.resblocks.6.mlp.c_proj.weight', 'model.transformer.resblocks.6.mlp.c_proj.bias', 'model.transformer.resblocks.6.ln_2.weight', 'model.transformer.resblocks.6.ln_2.bias', 'model.transformer.resblocks.7.attn.in_proj_weight', 'model.transformer.resblocks.7.attn.in_proj_bias', 'model.transformer.resblocks.7.attn.out_proj.weight', 'model.transformer.resblocks.7.attn.out_proj.bias', 'model.transformer.resblocks.7.ln_1.weight', 'model.transformer.resblocks.7.ln_1.bias', 'model.transformer.resblocks.7.mlp.c_fc.weight', 'model.transformer.resblocks.7.mlp.c_fc.bias', 'model.transformer.resblocks.7.mlp.c_proj.weight', 'model.transformer.resblocks.7.mlp.c_proj.bias', 'model.transformer.resblocks.7.ln_2.weight', 'model.transformer.resblocks.7.ln_2.bias', 'model.transformer.resblocks.8.attn.in_proj_weight', 'model.transformer.resblocks.8.attn.in_proj_bias', 'model.transformer.resblocks.8.attn.out_proj.weight', 'model.transformer.resblocks.8.attn.out_proj.bias', 'model.transformer.resblocks.8.ln_1.weight', 'model.transformer.resblocks.8.ln_1.bias', 'model.transformer.resblocks.8.mlp.c_fc.weight', 'model.transformer.resblocks.8.mlp.c_fc.bias', 'model.transformer.resblocks.8.mlp.c_proj.weight', 'model.transformer.resblocks.8.mlp.c_proj.bias', 'model.transformer.resblocks.8.ln_2.weight', 'model.transformer.resblocks.8.ln_2.bias', 'model.transformer.resblocks.9.attn.in_proj_weight', 'model.transformer.resblocks.9.attn.in_proj_bias', 'model.transformer.resblocks.9.attn.out_proj.weight', 'model.transformer.resblocks.9.attn.out_proj.bias', 'model.transformer.resblocks.9.ln_1.weight', 'model.transformer.resblocks.9.ln_1.bias', 'model.transformer.resblocks.9.mlp.c_fc.weight', 'model.transformer.resblocks.9.mlp.c_fc.bias', 'model.transformer.resblocks.9.mlp.c_proj.weight', 'model.transformer.resblocks.9.mlp.c_proj.bias', 'model.transformer.resblocks.9.ln_2.weight', 'model.transformer.resblocks.9.ln_2.bias', 'model.transformer.resblocks.10.attn.in_proj_weight', 'model.transformer.resblocks.10.attn.in_proj_bias', 'model.transformer.resblocks.10.attn.out_proj.weight', 'model.transformer.resblocks.10.attn.out_proj.bias', 'model.transformer.resblocks.10.ln_1.weight', 'model.transformer.resblocks.10.ln_1.bias', 'model.transformer.resblocks.10.mlp.c_fc.weight', 'model.transformer.resblocks.10.mlp.c_fc.bias', 'model.transformer.resblocks.10.mlp.c_proj.weight', 'model.transformer.resblocks.10.mlp.c_proj.bias', 'model.transformer.resblocks.10.ln_2.weight', 'model.transformer.resblocks.10.ln_2.bias', 'model.transformer.resblocks.11.attn.in_proj_weight', 'model.transformer.resblocks.11.attn.in_proj_bias', 'model.transformer.resblocks.11.attn.out_proj.weight', 'model.transformer.resblocks.11.attn.out_proj.bias', 'model.transformer.resblocks.11.ln_1.weight', 'model.transformer.resblocks.11.ln_1.bias', 'model.transformer.resblocks.11.mlp.c_fc.weight', 'model.transformer.resblocks.11.mlp.c_fc.bias', 'model.transformer.resblocks.11.mlp.c_proj.weight', 'model.transformer.resblocks.11.mlp.c_proj.bias', 'model.transformer.resblocks.11.ln_2.weight', 'model.transformer.resblocks.11.ln_2.bias', 'model.token_embedding.weight', 'model.ln_final.weight', 'model.ln_final.bias']

# 파라미터 동결
for name, param in model.named_parameters():
    if name not in frozen_params:
        param.requires_grad_(False)

# Training settings
optimizer = optim.AdamW(model.parameters(), lr=args.learning_rate)
criterion = nn.CrossEntropyLoss()
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.step_size, gamma=args.gamma)

# Train model
train_model(model, train_dataloader, val_dataloader, criterion, optimizer, scheduler, args.epochs, device, x_test)
